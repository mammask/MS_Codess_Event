---
title: "Time Series Forecasting with Applications in R"
output: github_document
author: Kostas Mammas - email <komammas@microsoft.com>
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=8, fig.height=8, fig.align = 'center')
```

## Introduction to time series

Time series are analyzed to understand the past and predict the future enabling decision makers to make informed decisions. They are used in everyday operational decisions across numerous industries, including, finance, the environment .

In this workshop we will go through some basic concepts of statistical forecasting, starting from stationary and extending our journey to non-stationary time series using `R`.

## Loading and visualizing time series data

```{r fig1, fig.height = 5, fig.width = 8}
# Load required libraries
source("R/load_lib.R")

using("data.table", "ggplot2", "forecast","MLmetrics","gridExtra","tseries")

# Load data
dseries = fread("../data/DailyDelhiClimateTrain.csv",sep = ",")

# Access top 6 records
head(dseries)

# Convert to date format
dseries[, date := as.Date(date, format = "%Y-%m-%d")]

# Plot mean temperature
ggplot(data = dseries) + geom_line(aes(x = date, y = meantemp)) + xlab("Day") + ylab("Mean Temperature")
```

```{r}
# Create summary statistics of the mean temperature series
dseries[, summary(meantemp)]
```

```{r}
# Generate summary statistics by month
dseries[, .(`Min Mean Temperature` = min(meantemp),
            `Max Mean Temperature` = max(meantemp),
            `SD Mean Temperature` = sd(meantemp)
            ), by = month(date)]
```

```{r fig2, fig.height = 5, fig.width = 8}
# Create box plots for each month
dseries[, month := month(date)]
ggplot(data = dseries) +
  geom_boxplot(aes(x = factor(month(date)), y = meantemp)) +
  xlab("Month") +
  ylab("Mean Temperature")
```

```{r fig3, fig.height = 5, fig.width = 8}
# Create box plots per year
# dseries[, month := month(date)]
ggplot(data = dseries) +
  geom_boxplot(aes(x = factor(month(date)), y = meantemp)) +
  xlab("Month") +
  ylab("Mean Temperature") + facet_grid(~year(date))
```

## Stationary Time Series

A stationary time series is one whose properties do not depend on time at which time series is observed. Thus, time series with trends, or seasonality are not stationary, the trend and seasonality will affect the value of the time series at different times.

In general, stationary time series will not have predictable patterns in the long term. Time plots will show the series to be roughly horizontal (cyclic behavior is possible), with constant variance.

![](images/stationary-1.png)


Obvious seasonality rules out series (d), (h) and (i). Trends and changing levels rules out series (a), (c), (e), (f) and (i). Increasing variance also rules out (i). That leaves only (b) and (g) as stationary series.

### Definition: Strictly Stationary

We represent a time series of length $n$ by $\{x_{t} :t=1, ...,n\}=\{x_{1},x_{2},...,x_{n}\}$. It consists of $n$ values sampled at discrete times $1,2,...,n$. Strictly stationary is a time series for which the probabilistic behavior of every collection of values ${x_{1},x_{2},...,x_{n}}$ is identical to that of the time shifted set ${x_{1+h},x_{2+h},...,x_{n+h}}$. That is:

$$ \mathrm{Pr}\{x_{1} \leq c\_{1},...,x_{n} \leq c_{n}\} = \mathrm{Pr} \{x_{1+h} \leq c_{1},...,x_{n+h} \leq c_{n}\} $$

### Definition: Weakly Stationary

A weakly stationary time series $x_{t}$ is a finite variance process such that:

1.  The mean value function $\mu_{t}$ is constant and does not depend on time $t$, and
2.  The autocovariance function depends on $s$ and $t$ only through their difference $s-t$:

$$ \gamma_{x}(s,t) = \mathrm{cov}(x_{s},x_{t}) = \mathrm{E}[(x_{s}-\mu_{s})(x_{t}-\mu_{st})] $$

for all $S$ and $t$. The autocovariance measures the linear dependence between two points on the same series observed at different times.

## Differencing 

One way of conrting non stationary time series stationary, is by differencing. This requires computing the differences between consecutive observations. 

## Time Series Decomposition

Time series exhibit a variety  of patterns and it is often helpful to split the time series into three different components (trend, seasonality and cycles).

Many time series are dominated by a trend and/ or seasonal effects. A simple *additive decomposition* model is given by:

$$ x_{t} = m_{t} + s_{t} + z_{t} $$

where at time $t$, $x\_{t}$ is the observed series, $m\_{t}$ is the trend, $s\_{t}$ is the seasonal effect and $z\_{t}$ is an error term that is a sequence of correlated random variables with mean equal to zero. 


If the seasonal effects tend to increase as the trend increases as well, a *multiplicative* model may be more appropriate:

$$ x_{t} = m_{t} \times s_{t} + z_{t} $$

### Classical decomposition

```{r fig4, fig.height = 5, fig.width = 8}
# Convert mean temperature to time series object
tseries = ts(dseries[, meantemp],frequency = 12, start = dseries[,min(date)])

# Perform classical addive time series decomposition
autoplot(decompose(tseries, type = "additive")) +xlab("Date")
```


### LOESS Time Series Decomposition

```{r fig5, fig.height = 5, fig.width = 8}
autoplot(stl(tseries,s.window="periodic", robust=TRUE)) +xlab("Date") + ggtitle("STL Decomposition") + xlab("Date")
```


**Autocorrelation**: It indicates the correlation of the time series with prior time steps and provides information around two different aspects of the time series:

1.  It shows whether the lagged values of the time series influence the current step
2.  It provides information around the stationarity of the time series. A non-stationary time series has autocorrelation lags that fall to zero very quickly.

It is clear from the autocorrelation plot that the lags of the mean temperature are statistically significant, therefore they lead to the conclusion that the mean temperature series are highly correlated**.**

**Partial autocorrelation**: Is the correlation between the correlation of the time series with prior time steps with the relationships of the intervening observations removed.

```{r fig6, fig.height = 6, fig.width = 8}
# Check autocorrelation in time series
gridExtra::grid.arrange(
  ggAcf(tseries) + ggtitle("Autocorrelation plot"),
  ggPacf(tseries) + ggtitle("Partial autocorrelation plot")
)
```

## Forecasting and Model Selection (Statistical Approach)

We split the time series data into training and test. The training set is used to train the model using a set of hyper-parameters and the test set is used to validate the performance of the selected hyper-parameters test set.

![](images/split.png)

The training set will be used primarily to understand the time series and their characteristics. We don't want to learn the characteristics of the time series in the validation and test sets as this will violate the learning process and will induce information leakage in the training set.

When limited data is available, which is quite common in time series forecasting applications, multiple models are fitted in the training set, the best model is selected using the AIC or the BIC criterion and the best model is evaluated in the test set. I

```{r fig7, fig.height = 5, fig.width = 8}
# Obtain the training set only
train_ratio = 0.8
test_ratio  = 0.2

train_data = copy(dseries[1:round(train_ratio*.N)])
test_data  = copy(dseries[(round(train_ratio*.N)+1) : .N])

# Convert to time series object
tseries = ts(train_data[,meantemp], start  = train_data[,min(date)], frequency = 12)
# Decompose time series
decomp  = stl(tseries, s.window = "periodic")

# Plot decomposed time series using LOESS
autoplot(decomp)
```


### **Stationarity Tests**

Before fitting forecasting models, it is critical to check the stationarity of the time series. Two stationarity checks are described:

#### KPSS Test:

```{r}
kpss.test(x = tseries)
```

According to the results of the \`KPSS\` test, we reject the null hypothesis and therefore the series is stationary.

#### ADF-Test:

```{r fig8, fig.height = 3, fig.width = 8}
# Perfroming adf test
adf.test(x = tseries)
```

According to the results of the ADF test, we fail to reject the null hypothesis and therefore the series is stationary.

## Holt Winters

....

## Autoregressive Integrated Moving Average

In this section, we introduce a special family of the stochastic linear models, the Autoregressive Integrated Moving Average (ARIMA).

### Model Training

```{r}
# Function to split the dataset into training validation and test set
train_valid_test_split <- function(x, trainSplit, validSplit, testSplit){

  status <- rep(as.character(NA), times = length(x))
  trainSize <- round(trainSplit * length(x))
  validSize <- round(validSplit * length(x))
  testSize  <- round(testSplit * length(x))
  
  status[1:trainSize]                            <- "Train"
  status[(trainSize+1):(trainSize+validSize)]    <- "Validation"
  status[(trainSize+validSize+1):length(status)] <- "Test"
  
  return(status)
}

# Split the dataset into training (0.6), validation (0.2) and test (0.2) 
dseries[, Split := train_valid_test_split(date, 0.6, 0.2, 0.2)]
```

```{r fig 7,  fig.height = 4, fig.width = 8}
# Convert to time series object
tseries = ts(dseries[Split == "Train",meantemp], start  = train_data[,min(date)], frequency = 12)

# Fit an auto arima in the training set
model = auto.arima(y = tseries, d = 0, max.p = 6, max.q = 6)
summary(model)

# Obtain fitted values
pred_train = model$fitted

# Obtain the mean temperature values in the training + the model predicted values in the training set
train_performance = copy(dseries[Split == "Train", .(date, meantemp)])
train_performance[, Predicted := pred_train]

# Plot actual versus predicted series in the training set
ggplot(data = train_performance) +
  geom_line(aes(x = date, y = meantemp)) +
  geom_line(aes(x = date, y = Predicted), color = "red", linetype = "dashed") +
  ggtitle("Actual versus Predicted Mean Temperature","Training set")
```

```{r}
# Model performance in the training set
train_performance[, .(MAPE = MLmetrics::MAPE(y_pred = meantemp, y_true = Predicted),
                    R2 = MLmetrics::R2_Score(y_pred = meantemp, y_true = Predicted),
                  RMSE = MLmetrics::RMSE(y_pred = meantemp, y_true = Predicted)
)
                  ]
```

#### Model Validation

In this step we perform model validation in the validation set. At each step we update the model and predict the next step.

```{r}
# Perform walk forward validation in the validation set
stepsAhead = 1
dseries[, id := 1:.N]

# Define the start/ end point of walk forward validation
startingPoint = dseries[Split == "Validation", min(id)-stepsAhead]
endPoint = dseries[Split == "Validation", max(id)-stepsAhead]

# Loop over each time step
pred_valid = c()
for (i in startingPoint:endPoint){
  # Get time series object
  dseriesupd = dseries[1:i, ts(data = meantemp,
                               start = min(date),  
                               frequency = 12
                               )
                       ]
  
  updatedModel = Arima(dseriesupd, model = model)
  pred_valid = c(pred_valid,predict(updatedModel, n.ahead = stepsAhead)[["pred"]][[1]])
}

# Obtain the mean temperature values in the training + the model predicted values in the training set
valid_performance = copy(dseries[Split == "Validation", .(date, meantemp)])
valid_performance[, Predicted := pred_valid]

# Plot actual versus predicted series in the training set
ggplot(data = valid_performance) +
  geom_line(aes(x = date, y = meantemp)) +
  geom_line(aes(x = date, y = Predicted), color = "red", linetype = "dashed") +
  ggtitle("Actual versus Predicted Mean Temperature","Validation set")
```

```{r}
valid_performance[, .(MAPE = MLmetrics::MAPE(y_pred = meantemp, y_true = Predicted),
                    R2 = MLmetrics::R2_Score(y_pred = meantemp, y_true = Predicted),
                  RMSE = MLmetrics::RMSE(y_pred = meantemp, y_true = Predicted)
)
                  ]
```

#### A Machine Learning approach to train validate and test the ARIMA model

We split the time series data into training, validation and test. The training set is used to train the model using a set of hyper-parameters, the validation set is used to validate the performance of the selected hyper-parameters in the validation set and the test set is used to test the generalization capability of the model using unseen data.

![](images/split-01.png)




#### A multivariate approach to time series forecasting using wavelet
